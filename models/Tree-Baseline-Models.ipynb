{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Classifier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# Define a function to convert SMILES to fingerprints\n",
    "def smiles_to_fingerprints(smiles, n_bits=1024):\n",
    "    \"\"\"\n",
    "    Converts a SMILES string to a molecular fingerprint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "        A SMILES string representing the molecular structure.\n",
    "    n_bits : int, Optional, default: 1024\n",
    "        The number of bits in the fingerprint.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of integers representing the molecular fingerprint.\n",
    "        Returns None if the SMILES string is invalid.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2,fpSize=n_bits)\n",
    "        return mfpgen.GetFingerprint(mol)\n",
    "    else:\n",
    "        return None # Return a zero vector if the SMILES is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "def return_scores(all_y_pred, all_y_true, y, title=\"Define Title\"):\n",
    "\n",
    "    # Get unique classes\n",
    "    classes = np.unique(all_y_true)\n",
    "\n",
    "    # Binarize the true and predicted labels for multi-class AUC calculation\n",
    "    y_true_bin = label_binarize(all_y_true, classes=classes)\n",
    "    y_pred_bin = label_binarize(all_y_pred, classes=classes)\n",
    "\n",
    "    # Calculate AUC for each class using One-vs-Rest (OvR) method\n",
    "  \n",
    "    # Average AUC across all classes\n",
    "    avg_auc = roc_auc_score(y_true_bin, y_pred_bin, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "    # Weighted AUC, which considers the support of each class\n",
    "    weighted_auc = roc_auc_score(y_true_bin, y_pred_bin, average=\"weighted\", multi_class=\"ovr\")\n",
    "\n",
    "\n",
    "    print(f\"{title}\")\n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "\n",
    "    # Print classification report for per-class metrics\n",
    "    print(\"\\nPer-Class Classification Report:\")\n",
    "    print(classification_report(all_y_true, all_y_pred, target_names=np.unique(y), digits=4))\n",
    "\n",
    "    # Calculate weighted and macro averages for precision, recall, and F1 score\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        all_y_true, all_y_pred, average='weighted'\n",
    "    )\n",
    "\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        all_y_true, all_y_pred, average='macro'\n",
    "    )\n",
    "\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "    # Print weighted averages\n",
    "    print(\"\\nWeighted Averages:\")\n",
    "    print(f\"Weighted Precision: {precision_weighted:.4f}\")\n",
    "    print(f\"Weighted Recall: {recall_weighted:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {f1_weighted:.4f}\")\n",
    "    print(f\"Weighted Average AUC: {weighted_auc:.4f}\")\n",
    "\n",
    "    # Print unweighted (macro) averages\n",
    "    print(\"\\nMacro Averages (Unweighted):\")\n",
    "    print(f\"Macro Precision: {precision_macro:.4f}\")\n",
    "    print(f\"Macro Recall: {recall_macro:.4f}\")\n",
    "    print(f\"Macro F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"Average AUC (macro): {avg_auc:.4f}\")\n",
    "\n",
    "    # Optional: Calculate per-class precision, recall, and F1 score explicitly\n",
    "    # Get per-class metrics using precision_recall_fscore_support without averaging\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        all_y_true, all_y_pred, average=None, labels=np.unique(all_y_true)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost on fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:45:10] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:45:10] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:45:10] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:45:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:45:12] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv(\"fart_train.csv\")\n",
    "val = pd.read_csv(\"fart_val.csv\")\n",
    "test = pd.read_csv(\"fart_test.csv\")\n",
    "\n",
    "# Calculate fingerprints for train, val, and test sets separately\n",
    "train[\"fingerprints\"] = train[\"Canonicalized SMILES\"].apply(smiles_to_fingerprints)\n",
    "val[\"fingerprints\"] = val[\"Canonicalized SMILES\"].apply(smiles_to_fingerprints)\n",
    "test[\"fingerprints\"] = test[\"Canonicalized SMILES\"].apply(smiles_to_fingerprints)\n",
    "\n",
    "# Convert features list to a numpy array for modeling\n",
    "X_train = np.array(train['fingerprints'].tolist())\n",
    "y_train = train['Canonicalized Taste'].values\n",
    "\n",
    "X_val = np.array(val['fingerprints'].tolist())\n",
    "y_val = val['Canonicalized Taste'].values\n",
    "\n",
    "X_test = np.array(test['fingerprints'].tolist())\n",
    "y_test = test['Canonicalized Taste'].values\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the labels to integers\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_val_encoded = encoder.transform(y_val)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 15, 'learning_rate': 0.1, 'subsample': 0.6, 'val_accuracy': 0.9028393966282166}\n",
      "XGBoost on fingerprints\n",
      "\n",
      "Per-Class Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bitter     0.8528    0.7210    0.7814       233\n",
      "        sour     0.8893    0.9118    0.9004       238\n",
      "       sweet     0.9434    0.9511    0.9473      1473\n",
      "       umami     0.6667    0.3333    0.4444         6\n",
      "   undefined     0.7323    0.7829    0.7568       304\n",
      "\n",
      "    accuracy                         0.8988      2254\n",
      "   macro avg     0.8169    0.7400    0.7661      2254\n",
      "weighted avg     0.8991    0.8988    0.8981      2254\n",
      "\n",
      "Overall Accuracy: 0.8988\n",
      "\n",
      "Weighted Averages:\n",
      "Weighted Precision: 0.8991\n",
      "Weighted Recall: 0.8988\n",
      "Weighted F1 Score: 0.8981\n",
      "Weighted Average AUC: 0.9098\n",
      "\n",
      "Macro Averages (Unweighted):\n",
      "Macro Precision: 0.8169\n",
      "Macro Recall: 0.7400\n",
      "Macro F1 Score: 0.7661\n",
      "Average AUC (macro): 0.8520\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define train and validation sets (you already have these as separate files)\n",
    "# X_train, y_train_encoded: training data\n",
    "# X_val, y_val_encoded: validation data\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [3, 5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Perform a grid search manually\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "            for subsample in param_grid['subsample']:\n",
    "                # Define the model with current hyperparameters\n",
    "                xgb = XGBClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    learning_rate=learning_rate,\n",
    "                    subsample=subsample,\n",
    "                    objective='multi:softprob',\n",
    "                    eval_metric='mlogloss',\n",
    "                    random_state=101,\n",
    "                    tree_method='hist'  # Use histogram-based algorithm for efficiency\n",
    "                )\n",
    "                \n",
    "                # Train the model on the train split\n",
    "                xgb.fit(X_train, y_train_encoded)\n",
    "                \n",
    "                # Validate the model on the validation split\n",
    "                y_val_pred = xgb.predict(X_val)\n",
    "                val_accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "                \n",
    "                # Store the results\n",
    "                results.append({\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'subsample': subsample,\n",
    "                    'val_accuracy': val_accuracy\n",
    "                })\n",
    "\n",
    "# Find the best hyperparameters based on validation accuracy\n",
    "best_params = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the best model on the train set\n",
    "best_xgb = XGBClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    subsample=best_params['subsample'],\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=101,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "# Train the best model\n",
    "best_xgb.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Test on the test set (if available)\n",
    "# Get probability predictions\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Call the return_scores function\n",
    "return_scores(y_pred, y_test_encoded, y_test, title=\"XGBoost on fingerprints\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 15, 'val_accuracy': 0.790150842945874}\n",
      "Balanced Random Forest\n",
      "\n",
      "Per-Class Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bitter     0.6852    0.4764    0.5620       233\n",
      "        sour     0.6145    0.8908    0.7273       238\n",
      "       sweet     0.9579    0.8344    0.8919      1473\n",
      "       umami     0.0909    0.6667    0.1600         6\n",
      "   undefined     0.5738    0.7928    0.6657       304\n",
      "\n",
      "    accuracy                         0.7972      2254\n",
      "   macro avg     0.5845    0.7322    0.6014      2254\n",
      "weighted avg     0.8393    0.7972    0.8079      2254\n",
      "\n",
      "Overall Accuracy: 0.7972\n",
      "\n",
      "Weighted Averages:\n",
      "Weighted Precision: 0.8393\n",
      "Weighted Recall: 0.7972\n",
      "Weighted F1 Score: 0.8079\n",
      "Weighted Average AUC: 0.8650\n",
      "\n",
      "Macro Averages (Unweighted):\n",
      "Macro Precision: 0.5845\n",
      "Macro Recall: 0.7322\n",
      "Macro F1 Score: 0.6014\n",
      "Average AUC (macro): 0.8391\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [3, 5, 10, 15]\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Perform a grid search manually\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        # Define the model with current hyperparameters and future behavior\n",
    "        brf = BalancedRandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            criterion=\"gini\",\n",
    "            random_state=101,\n",
    "            sampling_strategy='all',  # Future behavior: Use all classes equally\n",
    "            replacement=True,         # Future behavior: Replace samples\n",
    "            bootstrap=False            # Future behavior: Disable bootstrap sampling\n",
    "        )\n",
    "        \n",
    "        # Train the model on the train split\n",
    "        brf.fit(X_train, y_train_encoded)\n",
    "        \n",
    "        # Validate the model on the validation split\n",
    "        y_val_pred = brf.predict(X_val)\n",
    "        val_accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "        \n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "\n",
    "# Find the best hyperparameters based on validation accuracy\n",
    "best_params = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the best model on the train set with future behavior\n",
    "best_brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    criterion=\"gini\",\n",
    "    random_state=101,\n",
    "    sampling_strategy='all',\n",
    "    replacement=True,\n",
    "    bootstrap=False\n",
    ")\n",
    "\n",
    "# Train the best model\n",
    "best_brf.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Test on the test set\n",
    "y_pred = best_brf.predict(X_test)\n",
    "\n",
    "# Call the return_scores function\n",
    "return_scores(y_pred, y_test_encoded, y_test, title=\"Balanced Random Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost on fingerprints and 15 Mordred Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:50:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:50:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:50:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:50:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:50:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:50:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:50:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:50:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:50:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:51:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:51:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:51:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:51:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:51:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:51:08] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "from mordred import Calculator, descriptors  # Requires Python 3.10 or earlier\n",
    "from rdkit import Chem\n",
    "\n",
    "# Create a Mordred Calculator\n",
    "calc = Calculator()\n",
    "\n",
    "# Register specific descriptors to the calculator\n",
    "calc.register(descriptors.Autocorrelation.ATSC(0, 'c'))\n",
    "calc.register(descriptors.Autocorrelation.ATSC(0, 'se'))\n",
    "calc.register(descriptors.Autocorrelation.AATS(0, 'i'))\n",
    "calc.register(descriptors.Autocorrelation.ATSC(1, 'p'))\n",
    "calc.register(descriptors.Autocorrelation.AATSC(2, 'se'))\n",
    "calc.register(descriptors.Autocorrelation.AATSC(0, 'm'))\n",
    "calc.register(descriptors.Autocorrelation.AATSC(1, 'Z'))\n",
    "calc.register(descriptors.Autocorrelation.AATSC(2, 'are'))\n",
    "calc.register(descriptors.Autocorrelation.AATSC(1, 'pe'))\n",
    "calc.register(descriptors.AdjacencyMatrix.AdjacencyMatrix('SpDiam'))\n",
    "calc.register(descriptors.Autocorrelation.ATSC(1, 'c'))\n",
    "calc.register(descriptors.Autocorrelation.ATSC(1, 'se'))\n",
    "calc.register(descriptors.Autocorrelation.ATSC(1, 'Z'))\n",
    "calc.register(descriptors.Autocorrelation.ATSC(1, 'm'))\n",
    "calc.register(descriptors.Autocorrelation.ATSC(4, 's'))\n",
    "\n",
    "\n",
    "def generate_descriptors(smiles, calculator=calc):\n",
    "    \"\"\"\n",
    "    Calculates selected descriptors from SMILES.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "        A SMILES string representing the molecular structure.\n",
    "    calculator : Calculator\n",
    "        A Mordred Calculator with specified descriptors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of descriptor values or None if calculation fails.\n",
    "    \"\"\"\n",
    "    if calculator is None or smiles is None:\n",
    "        return None\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return list(calculator(mol))  # Convert the Mordred Result to a list\n",
    "    except Exception as error:\n",
    "        return None  # Handle errors gracefully\n",
    "\n",
    "\n",
    "# Apply descriptor generation separately for train, val, and test sets\n",
    "train['mordred_descriptors'] = train['Canonicalized SMILES'].apply(generate_descriptors)\n",
    "val['mordred_descriptors'] = val['Canonicalized SMILES'].apply(generate_descriptors)\n",
    "test['mordred_descriptors'] = test['Canonicalized SMILES'].apply(generate_descriptors)\n",
    "\n",
    "# Replace missing descriptors with zero vectors\n",
    "train['mordred_descriptors'] = train['mordred_descriptors'].apply(\n",
    "    lambda x: np.zeros(calc.size, dtype=np.float32) if x is None else np.array(x, dtype=np.float32)\n",
    ")\n",
    "val['mordred_descriptors'] = val['mordred_descriptors'].apply(\n",
    "    lambda x: np.zeros(calc.size, dtype=np.float32) if x is None else np.array(x, dtype=np.float32)\n",
    ")\n",
    "test['mordred_descriptors'] = test['mordred_descriptors'].apply(\n",
    "    lambda x: np.zeros(calc.size, dtype=np.float32) if x is None else np.array(x, dtype=np.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ensure Mordred descriptors and fingerprints are in compatible formats\n",
    "# Convert fingerprints (ExplicitBitVect) to NumPy arrays\n",
    "train['fingerprints'] = train['fingerprints'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "val['fingerprints'] = val['fingerprints'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "test['fingerprints'] = test['fingerprints'].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "# Ensure Mordred descriptors are lists or arrays and fill missing descriptors with zeros\n",
    "train['mordred_descriptors'] = train['mordred_descriptors'].apply(\n",
    "    lambda x: np.array(x, dtype=np.float32) if x is not None else np.zeros(calc.size, dtype=np.float32)\n",
    ")\n",
    "val['mordred_descriptors'] = val['mordred_descriptors'].apply(\n",
    "    lambda x: np.array(x, dtype=np.float32) if x is not None else np.zeros(calc.size, dtype=np.float32)\n",
    ")\n",
    "test['mordred_descriptors'] = test['mordred_descriptors'].apply(\n",
    "    lambda x: np.array(x, dtype=np.float32) if x is not None else np.zeros(calc.size, dtype=np.float32)\n",
    ")\n",
    "\n",
    "# Combine fingerprints and descriptors for X_train\n",
    "X_train = np.array([\n",
    "    np.concatenate((fingerprint, descriptor))\n",
    "    for fingerprint, descriptor in zip(train['fingerprints'], train['mordred_descriptors'])\n",
    "])\n",
    "\n",
    "# Combine fingerprints and descriptors for X_val\n",
    "X_val = np.array([\n",
    "    np.concatenate((fingerprint, descriptor))\n",
    "    for fingerprint, descriptor in zip(val['fingerprints'], val['mordred_descriptors'])\n",
    "])\n",
    "\n",
    "# Combine fingerprints and descriptors for X_test\n",
    "X_test = np.array([\n",
    "    np.concatenate((fingerprint, descriptor))\n",
    "    for fingerprint, descriptor in zip(test['fingerprints'], test['mordred_descriptors'])\n",
    "])\n",
    "\n",
    "# Extract labels for train, val, and test sets\n",
    "y_train = train['Canonicalized Taste'].values\n",
    "y_val = val['Canonicalized Taste'].values\n",
    "y_test = test['Canonicalized Taste'].values\n",
    "\n",
    "# Initialize the LabelEncoder and encode labels for XGBoost\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_val_encoded = encoder.transform(y_val)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 200, 'max_depth': 15, 'learning_rate': 0.1, 'subsample': 0.8, 'val_accuracy': 0.9023957409050577}\n",
      "XGBoost on fingerprints+descriptors\n",
      "\n",
      "Per-Class Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bitter     0.8571    0.7468    0.7982       233\n",
      "        sour     0.9064    0.8950    0.9006       238\n",
      "       sweet     0.9394    0.9464    0.9428      1473\n",
      "       umami     1.0000    0.3333    0.5000         6\n",
      "   undefined     0.7182    0.7796    0.7476       304\n",
      "\n",
      "    accuracy                         0.8962      2254\n",
      "   macro avg     0.8842    0.7402    0.7779      2254\n",
      "weighted avg     0.8977    0.8962    0.8959      2254\n",
      "\n",
      "Overall Accuracy: 0.8962\n",
      "\n",
      "Weighted Averages:\n",
      "Weighted Precision: 0.8977\n",
      "Weighted Recall: 0.8962\n",
      "Weighted F1 Score: 0.8959\n",
      "Weighted Average AUC: 0.9059\n",
      "\n",
      "Macro Averages (Unweighted):\n",
      "Macro Precision: 0.8842\n",
      "Macro Recall: 0.7402\n",
      "Macro F1 Score: 0.7779\n",
      "Average AUC (macro): 0.8513\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [3, 5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Perform a grid search manually\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "            for subsample in param_grid['subsample']:\n",
    "                # Define the model with current hyperparameters\n",
    "                xgb = XGBClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    learning_rate=learning_rate,\n",
    "                    subsample=subsample,\n",
    "                    objective='multi:softprob',\n",
    "                    eval_metric='mlogloss',\n",
    "                    random_state=101,\n",
    "                    tree_method='hist'  # Use histogram-based algorithm for efficiency\n",
    "                )\n",
    "                \n",
    "                # Train the model on the train split\n",
    "                xgb.fit(X_train, y_train_encoded)\n",
    "                \n",
    "                # Validate the model on the validation split\n",
    "                y_val_pred = xgb.predict(X_val)\n",
    "                val_accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "                \n",
    "                # Store the results\n",
    "                results.append({\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'subsample': subsample,\n",
    "                    'val_accuracy': val_accuracy\n",
    "                })\n",
    "\n",
    "# Find the best hyperparameters based on validation accuracy\n",
    "best_params = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the best model on the train set\n",
    "best_xgb = XGBClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    subsample=best_params['subsample'],\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=101,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "# Train the best model\n",
    "best_xgb.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Test on the test set (if available)\n",
    "# Get probability predictions\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Call the return_scores function\n",
    "return_scores(y_pred, y_test_encoded, y_test, title=\"XGBoost on fingerprints+descriptors\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
